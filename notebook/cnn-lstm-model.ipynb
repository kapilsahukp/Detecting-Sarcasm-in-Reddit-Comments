{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T18:54:39.683106Z",
     "iopub.status.busy": "2021-12-07T18:54:39.682807Z",
     "iopub.status.idle": "2021-12-07T18:54:39.686957Z",
     "shell.execute_reply": "2021-12-07T18:54:39.685861Z",
     "shell.execute_reply.started": "2021-12-07T18:54:39.683071Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip uninstall tensorflow\n",
    "# ! pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T19:01:17.027220Z",
     "iopub.status.busy": "2021-12-07T19:01:17.026626Z",
     "iopub.status.idle": "2021-12-07T19:01:17.037344Z",
     "shell.execute_reply": "2021-12-07T19:01:17.036524Z",
     "shell.execute_reply.started": "2021-12-07T19:01:17.027179Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D,Dropout,LSTM,Bidirectional\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer,one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re,nltk,swifter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T18:05:14.178146Z",
     "iopub.status.busy": "2021-12-07T18:05:14.177887Z",
     "iopub.status.idle": "2021-12-07T18:05:16.228372Z",
     "shell.execute_reply": "2021-12-07T18:05:16.227452Z",
     "shell.execute_reply.started": "2021-12-07T18:05:14.178109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kapil\\AppData\\Local\\Temp/ipykernel_20156/1594334578.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking resources available for training:\n",
    "tf.test.is_gpu_available()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T18:05:16.230431Z",
     "iopub.status.busy": "2021-12-07T18:05:16.229883Z",
     "iopub.status.idle": "2021-12-07T18:05:23.571124Z",
     "shell.execute_reply": "2021-12-07T18:05:23.570411Z",
     "shell.execute_reply.started": "2021-12-07T18:05:16.230381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>-4</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>-8</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  score  \\\n",
       "0      0                                         NC and NH.  Trumpbart      2   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906     -4   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth      3   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha     -8   \n",
       "4      0                    I could use one of those tools.  cush2push      6   \n",
       "\n",
       "           created_utc                                     parent_comment  \n",
       "0  2016-10-16 23:55:23  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  2016-11-01 00:24:10  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2  2016-09-22 21:45:37                            They're favored to win.  \n",
       "3  2016-10-18 21:03:47                         deadass don't kill my buzz  \n",
       "4  2016-12-30 17:00:13  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For reading data, we have to put path of the csv data file.\n",
    "df = pd.read_csv(r'C:\\Users\\kapil\\OneDrive\\Desktop\\NLP_Uzair\\Project\\Reddit_Sarcasm_Detection-main\\Reddit_Sarcasm_Detection-main\\src\\data\\raw\\train-balanced-sarcasm.csv')\n",
    "df = df.fillna('')\n",
    "df = df[['label','comment','author','score','created_utc','parent_comment']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T18:05:23.572728Z",
     "iopub.status.busy": "2021-12-07T18:05:23.572372Z",
     "iopub.status.idle": "2021-12-07T18:08:06.017388Z",
     "shell.execute_reply": "2021-12-07T18:08:06.016332Z",
     "shell.execute_reply.started": "2021-12-07T18:05:23.572688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83f1667feb34943a9f4c16e3dcf92a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stops = set(stopwords.words('english')) - {'no','not','nor','against','above','below','off','own'}\n",
    "def clean_text(comment):\n",
    "    text = str(comment)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',' ',text)\n",
    "    text = re.sub(\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"[0-9]+\",\" \",text)\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\",\" \",text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"n\\'t\", ' not',text)\n",
    "    text = text.replace('\\\\r', ' ')\n",
    "    text = text.replace('\\\\\"', ' ')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    text = re.sub('[^A-Za-z0-9]+',' ', text)\n",
    "    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    text = ' '.join(token for token in tokenizer.tokenize(text.lower()) if token not in stops)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "#Swifter helps in speeding up the process\n",
    "df[\"cleaned_comment\"] = df.swifter.apply(lambda x: clean_text(x[\"comment\"]),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T18:08:06.019636Z",
     "iopub.status.busy": "2021-12-07T18:08:06.019335Z",
     "iopub.status.idle": "2021-12-07T18:08:07.401193Z",
     "shell.execute_reply": "2021-12-07T18:08:07.400441Z",
     "shell.execute_reply.started": "2021-12-07T18:08:06.019594Z"
    }
   },
   "outputs": [],
   "source": [
    "#80-20 split\n",
    "train_x, val_x,train_y , val_y = train_test_split(df.drop('label',axis=1),df['label'],random_state=123,test_size=0.20)\n",
    "train# Train-test Split_txt = train_x['cleaned_comment']\n",
    "val_tx# Cleaning commentst = val_x['cleaned_comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T18:08:07.402619Z",
     "iopub.status.busy": "2021-12-07T18:08:07.402344Z",
     "iopub.status.idle": "2021-12-07T18:08:29.752001Z",
     "shell.execute_reply": "2021-12-07T18:08:29.751224Z",
     "shell.execute_reply.started": "2021-12-07T18:08:07.402583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:137267\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=6000)\n",
    "tokenizer.fit_on_texts(train_txt)\n",
    "cnn_train = tokenizer.texts_to_sequences(train_txt)\n",
    "cnn_val = tokenizer.texts_to_sequences(val_txt)\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "print(f\"Vocab size:{vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T18:08:29.753697Z",
     "iopub.status.busy": "2021-12-07T18:08:29.753234Z",
     "iopub.status.idle": "2021-12-07T18:08:35.539992Z",
     "shell.execute_reply": "2021-12-07T18:08:35.539253Z",
     "shell.execute_reply.started": "2021-12-07T18:08:29.753653Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using post padding with max len 100\n",
    "maxlen = 100\n",
    "Xcnn_train = pad_sequences(cnn_train, padding='post', maxlen=maxlen)\n",
    "Xcnn_val = pad_sequences(cnn_val, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T18:08:35.542365Z",
     "iopub.status.busy": "2021-12-07T18:08:35.542167Z",
     "iopub.status.idle": "2021-12-07T18:08:35.963797Z",
     "shell.execute_reply": "2021-12-07T18:08:35.963046Z",
     "shell.execute_reply.started": "2021-12-07T18:08:35.542340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 200)          27453400  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 96, 128)           128128    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                2064      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,583,737\n",
      "Trainable params: 27,583,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 200\n",
    "#Defining type: Its Sequential here as it allows to build model layer by layer\n",
    "cnn_model = Sequential()\n",
    "#Define input\n",
    "cnn_model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "#Define convolution layer with activation function as \"Relu\"\n",
    "cnn_model.add(Conv1D(128, 5,activation = 'relu'))\n",
    "#Defining pooling method, its max pooling here\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "#Adding more dense layers\n",
    "cnn_model.add(Dense(16, activation='relu'))\n",
    "cnn_model.add(Dense(8, activation='relu'))\n",
    "#Final output layer\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "cnn_model.compile(optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "cnn_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T18:08:35.966734Z",
     "iopub.status.busy": "2021-12-07T18:08:35.965158Z",
     "iopub.status.idle": "2021-12-07T18:49:16.085570Z",
     "shell.execute_reply": "2021-12-07T18:49:16.084822Z",
     "shell.execute_reply.started": "2021-12-07T18:08:35.966687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "80866/80866 [==============================] - 48433s 599ms/step - loss: 0.5941 - accuracy: 0.6777 - val_loss: 0.5850 - val_accuracy: 0.6881\n",
      "Epoch 2/3\n",
      "80866/80866 [==============================] - 40876s 505ms/step - loss: 0.5678 - accuracy: 0.7017 - val_loss: 0.5786 - val_accuracy: 0.6922\n",
      "Epoch 3/3\n",
      "80866/80866 [==============================] - 49852s 616ms/step - loss: 0.5495 - accuracy: 0.7162 - val_loss: 0.5887 - val_accuracy: 0.6888\n",
      "Training Accuracy: 0.7349\n",
      "Testing Accuracy:  0.6888\n"
     ]
    }
   ],
   "source": [
    "#Fitting the model and calculating trian and test accuracies in each Epoch (for 3 epochs)\n",
    "cnn_model.fit(Xcnn_train, train_y,\n",
    "                    epochs=3,\n",
    "                    verbose=True,\n",
    "                    validation_data=(Xcnn_val, val_y),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = cnn_model.evaluate(Xcnn_train, train_y, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = cnn_model.evaluate(Xcnn_val, val_y, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy)) \n",
    "\n",
    "# Epoch 1/3\n",
    "# 80866/80866 [==============================] - 48433s 599ms/step - loss: 0.5941 - accuracy: 0.6777 - val_loss: 0.5850 - val_accuracy: 0.6881\n",
    "# Epoch 2/3\n",
    "# 80866/80866 [==============================] - 40876s 505ms/step - loss: 0.5678 - accuracy: 0.7017 - val_loss: 0.5786 - val_accuracy: 0.6922\n",
    "# Epoch 3/3\n",
    "# 80866/80866 [==============================] - 49852s 616ms/step - loss: 0.5495 - accuracy: 0.7162 - val_loss: 0.5887 - val_accuracy: 0.6888\n",
    "# Training Accuracy: 0.7349\n",
    "# Testing Accuracy:  0.6888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T21:50:59.775237Z",
     "iopub.status.busy": "2021-12-07T21:50:59.774963Z",
     "iopub.status.idle": "2021-12-07T21:53:50.731175Z",
     "shell.execute_reply": "2021-12-07T21:53:50.730257Z",
     "shell.execute_reply.started": "2021-12-07T21:50:59.775206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e069b84f2a4d20bd3245bfd7072e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Included all the words without any exception (i.e, stop words are not removed)\n",
    "stops_1 = {}\n",
    "def clean_text(comment):\n",
    "    text = str(comment)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',' ',text)\n",
    "    text = re.sub(\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"[0-9]+\",\" \",text)\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\",\" \",text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"n\\'t\", ' not',text)\n",
    "    text = text.replace('\\\\r', ' ')\n",
    "    text = text.replace('\\\\\"', ' ')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    text = re.sub('[^A-Za-z0-9]+',' ', text)\n",
    "    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    text = ' '.join(token for token in tokenizer.tokenize(text.lower()) if token not in stops_1)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "df[\"cleaned_comment_1\"] = df.swifter.apply(lambda x: clean_text(x[\"comment\"]),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T21:53:50.733662Z",
     "iopub.status.busy": "2021-12-07T21:53:50.733352Z",
     "iopub.status.idle": "2021-12-07T21:54:18.863377Z",
     "shell.execute_reply": "2021-12-07T21:54:18.862612Z",
     "shell.execute_reply.started": "2021-12-07T21:53:50.733623Z"
    }
   },
   "outputs": [],
   "source": [
    "#Getting corpus\n",
    "corpus = [df['cleaned_comment_1'][i] for i in range( len(df))]\n",
    "voc_size=5000\n",
    "\n",
    "#Using one_hot encoding\n",
    "onehot_=[one_hot(words,voc_size)for words in corpus] \n",
    "\n",
    "#Defining max sentence length\n",
    "max_sent_length = 80\n",
    "\n",
    "#Embedded_docs with pre added padding\n",
    "embedded_docs=pad_sequences(onehot_,padding='pre',maxlen=max_sent_length)\n",
    "    \n",
    "embedding_vector_features=80\n",
    "\n",
    "#Final X and y dataset\n",
    "X_final=np.array(embedded_docs)\n",
    "y_final=np.array(df['label'])\n",
    "\n",
    "#Train test split: (80-20)\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_final, y_final, test_size=0.20, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T21:54:18.866577Z",
     "iopub.status.busy": "2021-12-07T21:54:18.866287Z",
     "iopub.status.idle": "2021-12-07T21:54:19.319147Z",
     "shell.execute_reply": "2021-12-07T21:54:19.318258Z",
     "shell.execute_reply.started": "2021-12-07T21:54:18.866522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 80, 80)            400000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              214016    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 614,273\n",
      "Trainable params: 614,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Initializing and defining lstm with parameters\n",
    "#Its Sequential here as it allows to build model layer by layer\n",
    "lstm_model=Sequential()\n",
    "#Define input\n",
    "lstm_model.add(Embedding(voc_size,embedding_vector_features,input_length=max_sent_length))\n",
    "#Using bidirectional with 128 features\n",
    "lstm_model.add(Bidirectional(LSTM(128)))\n",
    "#Dropout value is 0.3\n",
    "lstm_model.add(Dropout(0.3))\n",
    "#To convert into 1D to cretae a single long vector\n",
    "lstm_model.add(Flatten())\n",
    "#Final output layer\n",
    "lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "#Compiling and getting summary with cross entropy loss and using adam optimizer\n",
    "lstm_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-07T21:54:19.321826Z",
     "iopub.status.busy": "2021-12-07T21:54:19.321309Z",
     "iopub.status.idle": "2021-12-08T00:18:43.386146Z",
     "shell.execute_reply": "2021-12-08T00:18:43.385448Z",
     "shell.execute_reply.started": "2021-12-07T21:54:19.321782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80866/80866 [==============================] - 4454s 55ms/step - loss: 0.5726 - accuracy: 0.6973 - val_loss: 0.5546 - val_accuracy: 0.7115\n",
      "Epoch 2/10\n",
      "80866/80866 [==============================] - 3596s 44ms/step - loss: 0.5449 - accuracy: 0.7202 - val_loss: 0.5502 - val_accuracy: 0.7147\n",
      "Epoch 3/10\n",
      "80866/80866 [==============================] - 3508s 43ms/step - loss: 0.5323 - accuracy: 0.7295 - val_loss: 0.5475 - val_accuracy: 0.7173\n",
      "Epoch 4/10\n",
      "80866/80866 [==============================] - 6336s 78ms/step - loss: 0.5238 - accuracy: 0.7356 - val_loss: 0.5505 - val_accuracy: 0.7170\n",
      "Epoch 5/10\n",
      "80866/80866 [==============================] - 6604s 82ms/step - loss: 0.5168 - accuracy: 0.7407 - val_loss: 0.5506 - val_accuracy: 0.7158\n",
      "Epoch 6/10\n",
      "80866/80866 [==============================] - 4104s 51ms/step - loss: 0.5121 - accuracy: 0.7445 - val_loss: 0.5534 - val_accuracy: 0.7164\n",
      "Epoch 7/10\n",
      "80866/80866 [==============================] - 3684s 46ms/step - loss: 0.5078 - accuracy: 0.7470 - val_loss: 0.5553 - val_accuracy: 0.7136\n",
      "Epoch 8/10\n",
      "80866/80866 [==============================] - 3683s 46ms/step - loss: 0.5052 - accuracy: 0.7488 - val_loss: 0.5539 - val_accuracy: 0.7151\n",
      "Epoch 9/10\n",
      "80866/80866 [==============================] - 36856s 456ms/step - loss: 0.5030 - accuracy: 0.7508 - val_loss: 0.5584 - val_accuracy: 0.7144\n",
      "Epoch 10/10\n",
      "80866/80866 [==============================] - 5988s 74ms/step - loss: 0.5016 - accuracy: 0.7518 - val_loss: 0.5553 - val_accuracy: 0.7143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b32278ad60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the model for 10 epochs with batch size of 10\n",
    "lstm_model.fit(X_train_lstm,y_train_lstm,validation_data=(X_test_lstm,y_test_lstm),epochs=10,batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/10\n",
    "80866/80866 [==============================] - 4454s 55ms/step - loss: 0.5726 - accuracy: 0.6973 - val_loss: 0.5546 - val_accuracy: 0.7115\n",
    "Epoch 2/10\n",
    "80866/80866 [==============================] - 3596s 44ms/step - loss: 0.5449 - accuracy: 0.7202 - val_loss: 0.5502 - val_accuracy: 0.7147\n",
    "Epoch 3/10\n",
    "80866/80866 [==============================] - 3508s 43ms/step - loss: 0.5323 - accuracy: 0.7295 - val_loss: 0.5475 - val_accuracy: 0.7173\n",
    "Epoch 4/10\n",
    "80866/80866 [==============================] - 6336s 78ms/step - loss: 0.5238 - accuracy: 0.7356 - val_loss: 0.5505 - val_accuracy: 0.7170\n",
    "Epoch 5/10\n",
    "80866/80866 [==============================] - 6604s 82ms/step - loss: 0.5168 - accuracy: 0.7407 - val_loss: 0.5506 - val_accuracy: 0.7158\n",
    "Epoch 6/10\n",
    "80866/80866 [==============================] - 4104s 51ms/step - loss: 0.5121 - accuracy: 0.7445 - val_loss: 0.5534 - val_accuracy: 0.7164\n",
    "Epoch 7/10\n",
    "80866/80866 [==============================] - 3684s 46ms/step - loss: 0.5078 - accuracy: 0.7470 - val_loss: 0.5553 - val_accuracy: 0.7136\n",
    "Epoch 8/10\n",
    "80866/80866 [==============================] - 3683s 46ms/step - loss: 0.5052 - accuracy: 0.7488 - val_loss: 0.5539 - val_accuracy: 0.7151\n",
    "Epoch 9/10\n",
    "80866/80866 [==============================] - 36856s 456ms/step - loss: 0.5030 - accuracy: 0.7508 - val_loss: 0.5584 - val_accuracy: 0.7144\n",
    "Epoch 10/10\n",
    "80866/80866 [==============================] - 5988s 74ms/step - loss: 0.5016 - accuracy: 0.7518 - val_loss: 0.5553 - val_accuracy: 0.7143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-08T00:26:35.764511Z",
     "iopub.status.busy": "2021-12-08T00:26:35.764179Z",
     "iopub.status.idle": "2021-12-08T00:28:45.157167Z",
     "shell.execute_reply": "2021-12-08T00:28:45.156376Z",
     "shell.execute_reply.started": "2021-12-08T00:26:35.764472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7624\n",
      "Testing Accuracy:  0.7143\n"
     ]
    }
   ],
   "source": [
    "#Calculating train and test losses and accuracies\n",
    "loss, accuracy = lstm_model.evaluate(X_train_lstm, y_train_lstm, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = lstm_model.evaluate(X_test_lstm, y_test_lstm, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Accuracy: 0.7624\n",
    "Testing Accuracy:  0.7143"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
