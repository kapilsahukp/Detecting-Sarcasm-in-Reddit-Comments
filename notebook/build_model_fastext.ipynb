{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\kapil\\onedrive\\desktop\\nlp_uzair\\project\\reddit_sarcasm_detection-main\\reddit_sarcasm_detection-main\\src\\data\\raw\\fasttext-0.9.2-cp39-cp39-win_amd64.whl\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\users\\kapil\\anaconda3\\lib\\site-packages (from fasttext==0.9.2) (2.9.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\kapil\\anaconda3\\lib\\site-packages (from fasttext==0.9.2) (58.0.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\kapil\\anaconda3\\lib\\site-packages (from fasttext==0.9.2) (1.20.3)\n",
      "Installing collected packages: fasttext\n",
      "Successfully installed fasttext-0.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install C:\\Users\\kapil\\OneDrive\\Desktop\\NLP_Uzair\\Project\\Reddit_Sarcasm_Detection-main\\Reddit_Sarcasm_Detection-main\\src\\data\\raw\\fasttext-0.9.2-cp39-cp39-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in c:\\users\\kapil\\anaconda3\\lib\\site-packages (0.9.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\kapil\\anaconda3\\lib\\site-packages (from fasttext) (58.0.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\kapil\\anaconda3\\lib\\site-packages (from fasttext) (1.20.3)\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\users\\kapil\\anaconda3\\lib\\site-packages (from fasttext) (2.9.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re,nltk,swifter\n",
    "import csv\n",
    "import fasttext\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from zeugma.embeddings import EmbeddingTransformer\n",
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>-4</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>-8</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  score  \\\n",
       "0      0                                         NC and NH.  Trumpbart      2   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906     -4   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth      3   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha     -8   \n",
       "4      0                    I could use one of those tools.  cush2push      6   \n",
       "\n",
       "           created_utc                                     parent_comment  \n",
       "0  2016-10-16 23:55:23  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  2016-11-01 00:24:10  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2  2016-09-22 21:45:37                            They're favored to win.  \n",
       "3  2016-10-18 21:03:47                         deadass don't kill my buzz  \n",
       "4  2016-12-30 17:00:13  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For reading data, we have to put path of the csv data file.\n",
    "df = pd.read_csv(r'C:\\Users\\kapil\\OneDrive\\Desktop\\NLP_Uzair\\Project\\Reddit_Sarcasm_Detection-main\\Reddit_Sarcasm_Detection-main\\src\\data\\train-balanced-sarcasm.csv')\n",
    "df = df.fillna('')\n",
    "df = df[['label','comment','author','score','created_utc','parent_comment']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['label','comment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fa52c5331344ed8b4814840fee5ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kapil\\AppData\\Local\\Temp/ipykernel_14664/1636029199.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1[\"cleaned_comment\"] = df1.swifter.apply(lambda x: clean_text(x[\"comment\"]),axis=1)\n"
     ]
    }
   ],
   "source": [
    "#Set of stop words excluding some of the words which show negation since its necessary for sarcasm detection:\n",
    "stops = set(stopwords.words('english')) - {'no','not','nor','against','above','below','off','own'}\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "def clean_text(comment):\n",
    "    text = str(comment)# Cleaning comments\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',' ',text)\n",
    "    text = re.sub(\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"[0-9]+\",\" \",text)\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\",\" \",text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"n\\'t\", ' not',text)\n",
    "    text = text.replace('\\\\r', ' ')\n",
    "    text = text.replace('\\\\\"', ' ')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    text = re.sub('[^A-Za-z0-9]+',' ', text)\n",
    "    text = ' '.join(token for token in tokenizer.tokenize(text.lower()) if token not in stops)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "df1[\"cleaned_comment\"] = df1.swifter.apply(lambda x: clean_text(x[\"comment\"]),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>472809</th>\n",
       "      <td>looks like kind ficus fig tree could not tell</td>\n",
       "      <td>__label__0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59081</th>\n",
       "      <td>donald reality tv star care tv ratings</td>\n",
       "      <td>__label__0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5664</th>\n",
       "      <td>irl</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600959</th>\n",
       "      <td>yo creia que su programa consistia en ser vene...</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961633</th>\n",
       "      <td>like one seller selling new no box</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          cleaned_comment       label\n",
       "472809      looks like kind ficus fig tree could not tell  __label__0\n",
       "59081              donald reality tv star care tv ratings  __label__0\n",
       "5664                                                  irl  __label__1\n",
       "600959  yo creia que su programa consistia en ser vene...  __label__1\n",
       "961633                 like one seller selling new no box  __label__1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test Split\n",
    "train_x, val_x,train_y , val_y = train_test_split(df1.drop('label',axis=1),df['label'],random_state=123,test_size=0.20)\n",
    "train_txt = train_x['cleaned_comment']\n",
    "val_txt = val_x['cleaned_comment']\n",
    "train = pd.concat([train_x, train_y], axis=1)\n",
    "val = pd.concat([val_x, val_y], axis=1)\n",
    "train.drop(['comment'],axis=1,inplace=True)\n",
    "val.drop(['comment'],axis=1,inplace=True)\n",
    "\n",
    "#assigining label values as its needed for training with fasttext\n",
    "train.iloc[:, 1] = train.iloc[:, 1].apply(lambda x: '__label__' + str(x))\n",
    "val.iloc[:, 1] = val.iloc[:, 1].apply(lambda x: '__label__' + str(x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting to csv file\n",
    "train.to_csv(r\"C:\\Users\\kapil\\OneDrive\\Desktop\\NLP_Uzair\\Project\\Reddit_Sarcasm_Detection-main\\Reddit_Sarcasm_Detection-main\\src\\data\\raw\\fasttext_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting to csv file\n",
    "val.to_csv(r\"C:\\Users\\kapil\\OneDrive\\Desktop\\NLP_Uzair\\Project\\Reddit_Sarcasm_Detection-main\\Reddit_Sarcasm_Detection-main\\src\\data\\raw\\fasttext_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining export path:\n",
    "fasttext_train_path = r'C:\\Users\\kapil\\OneDrive\\Desktop\\NLP_Uzair\\Project\\Reddit_Sarcasm_Detection-main\\Reddit_Sarcasm_Detection-main\\src\\data\\raw\\fasttext_train.txt'\n",
    "fasttext_val_path = r'C:\\Users\\kapil\\OneDrive\\Desktop\\NLP_Uzair\\Project\\Reddit_Sarcasm_Detection-main\\Reddit_Sarcasm_Detection-main\\src\\data\\raw\\fasttext_val.txt'\n",
    "\n",
    "#Exporting data to csv\n",
    "train[['label','cleaned_comment']].to_csv(fasttext_train_path, \n",
    "                                            index = False, sep = ' ',header = None, quoting = csv.QUOTE_NONE,\n",
    "                                            quotechar = \"\", escapechar = \" \")\n",
    "val[['label','cleaned_comment']].to_csv(fasttext_val_path, \n",
    "                                            index = False, sep = ' ',header = None, quoting = csv.QUOTE_NONE,\n",
    "                                            quotechar = \"\", escapechar = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining pretrained vector path:\n",
    "prerained_vec_path = r'C:\\Users\\kapil\\OneDrive\\Desktop\\NLP_Uzair\\Project\\Reddit_Sarcasm_Detection-main\\Reddit_Sarcasm_Detection-main\\src\\data\\raw\\wiki-news-300d-1M.vec'\n",
    "#training model:\n",
    "model = fasttext.train_supervised(fasttext_train_path, wordNgrams = 2, \n",
    "                                        epoch = 4, verbose = 1, loss = 'softmax',\n",
    "                                        lr=0.099, ws = 8,dim=300,\n",
    "                                        pretrainedVectors = prerained_vec_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202166, 0.6899725967769061, 0.6899725967769061)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting test results containing (dataset size, precision, recall)\n",
    "model.test(fasttext_val_path)  \n",
    "#(200935, 0.6887451165799886, 0.6887451165799886)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36f0e4b0a7ea5d9009060c84ac0572df4a90298368e23ad6ca51e547eda0a6da"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
